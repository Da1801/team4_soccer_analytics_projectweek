{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Using XGBoost for Defensive Transitions (Defensive Success - Whether the team successfully recovers the ball through a defensive action (tackle, interception, clearance) within 10 seconds after losing possession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "df_transitions = pd.read_csv('all_teams_transition_data.csv')\n",
    "\n",
    "# Print the first few rows to understand the data structure\n",
    "print(\"DataFrame preview:\")\n",
    "print(df_transitions.head())\n",
    "\n",
    "# Check available columns\n",
    "print(\"\\nAvailable columns:\", df_transitions.columns.tolist())\n",
    "\n",
    "# Modify the prepare_features function to be more careful\n",
    "def prepare_features(df):\n",
    "    # Make a copy to avoid modifying the original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Create derived features based on columns that actually exist\n",
    "    derived_columns = []\n",
    "    \n",
    "    # Distance to goal (if loss_x exists)\n",
    "    if 'loss_x' in df.columns and 'period_id' in df.columns:\n",
    "        df['distance_to_goal'] = np.where(\n",
    "            df['period_id'] % 2 == 1,  # Odd periods (1,3)\n",
    "            105 - df['loss_x'],         # Distance from right goal\n",
    "            df['loss_x']                # Distance from left goal\n",
    "        )\n",
    "        derived_columns.append('distance_to_goal')\n",
    "    \n",
    "    # Create pitch zone features (if loss_x exists)\n",
    "    if 'loss_x' in df.columns:\n",
    "        try:\n",
    "            df['zone_x'] = pd.cut(\n",
    "                df['loss_x'], \n",
    "                bins=[0, 35, 70, 105], \n",
    "                labels=['Defensive', 'Middle', 'Attacking']\n",
    "            )\n",
    "            derived_columns.append('zone_x')\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating zone_x: {e}\")\n",
    "            # Check if loss_x contains valid numeric data\n",
    "            print(\"loss_x statistics:\", df['loss_x'].describe())\n",
    "    \n",
    "    # Create time features (if loss_time exists)\n",
    "    if 'loss_time' in df.columns:\n",
    "        df['game_minute'] = df['loss_time'] / 60  # Convert to minutes\n",
    "        derived_columns.append('game_minute')\n",
    "    \n",
    "    print(f\"Successfully created {len(derived_columns)} derived columns: {derived_columns}\")\n",
    "    return df\n",
    "\n",
    "# Prepare the data with the columns you actually have\n",
    "df = prepare_features(df_transitions)\n",
    "\n",
    "# Define features based on ONLY columns that actually exist in your dataframe\n",
    "available_columns = set(df.columns)\n",
    "print(\"\\nColumns after feature engineering:\", available_columns)\n",
    "\n",
    "# Define basic features that should be available\n",
    "possible_features = [\n",
    "    'loss_x', 'loss_y', \n",
    "    'period_id', 'loss_time',\n",
    "    'distance_to_goal', 'game_minute'\n",
    "]\n",
    "\n",
    "# Use only features that actually exist in the dataframe\n",
    "features = [f for f in possible_features if f in available_columns]\n",
    "print(\"\\nFeatures being used:\", features)\n",
    "\n",
    "# Check if we have the target variable\n",
    "target = 'defensive_success'\n",
    "if target not in df.columns:\n",
    "    print(f\"\\nError: Target column '{target}' not found in DataFrame\")\n",
    "    print(\"Available columns for potential target:\", df.columns.tolist())\n",
    "    \n",
    "    # Try to create the target if we have the necessary columns\n",
    "    if 'result' in df.columns:\n",
    "        print(\"Using 'result' column as proxy for defensive success\")\n",
    "        df[target] = (df['result'] == '1') | (df['result'] == 1) | (df['result'] == True)\n",
    "    else:\n",
    "        print(\"Cannot create target variable - stopping execution\")\n",
    "        raise KeyError(f\"Target column '{target}' not available and cannot be created\")\n",
    "else:\n",
    "    print(f\"\\nTarget column '{target}' found\")\n",
    "\n",
    "# Now continue with the modeling\n",
    "print(\"\\nPreparing to train model...\")\n",
    "\n",
    "# Drop rows with missing values in the features or target\n",
    "df_clean = df.dropna(subset=features + [target])\n",
    "print(f\"Data after removing NaNs: {len(df_clean)} rows (dropped {len(df) - len(df_clean)} rows)\")\n",
    "\n",
    "# Check if we have enough data\n",
    "if len(df_clean) < 10:\n",
    "    print(f\"Warning: Only {len(df_clean)} samples after removing NaN values - not enough to train a model\")\n",
    "else:\n",
    "    # Split the data\n",
    "    X = df_clean[features]\n",
    "    y = df_clean[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "    \n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "    \n",
    "    # Train XGBoost model\n",
    "    print(\"\\nTraining XGBoost model...\")\n",
    "    model = xgb.XGBClassifier(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='binary:logistic',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(f\"\\nTest accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    print(\"\\nModel training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Promising Data to Cluster and Analyze using K-Means Cluster (didn't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"all_teams_transition_data.csv\")\n",
    "\n",
    "# =====================================================================\n",
    "# 1. Select Behavioral Features\n",
    "# =====================================================================\n",
    "# Selected features for meaningful transition clusters\n",
    "numerical_features = [\n",
    "    'loss_x', 'loss_y', 'seconds_after_loss', 'time_to_defensive_action'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'movement_direction', 'x_sector', 'y_sector',\n",
    "    'successful_defensive_action', 'defensive_success'\n",
    "]\n",
    "\n",
    "# Group by team_id first to analyze team-level patterns\n",
    "team_transition_stats = df.groupby('team_id').agg({\n",
    "    'movement_direction': lambda x: (x == 'FORWARD').mean(),  # % of forward movements\n",
    "    'successful_defensive_action': 'mean',  # Success rate\n",
    "    'time_to_defensive_action': 'mean',  # Avg recovery time\n",
    "    'defensive_success': 'mean',  # Overall defensive success rate\n",
    "}).reset_index()\n",
    "\n",
    "# =====================================================================\n",
    "# 2. Preprocess Data (Handle Missing Values + Scale + Encode)\n",
    "# =====================================================================\n",
    "numeric_imputer = SimpleImputer(strategy=\"mean\")  # Fill NaN in numeric cols with mean\n",
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\")  # Fill NaN in categorical cols with mode\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([(\"imputer\", numeric_imputer), (\"scaler\", StandardScaler())]), numerical_features),\n",
    "        ('cat', Pipeline([(\"imputer\", categorical_imputer), (\"encoder\", OneHotEncoder(sparse_output=False, handle_unknown='ignore'))]), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_processed = preprocessor.fit_transform(df)\n",
    "\n",
    "# =====================================================================\n",
    "# 3. Determine Optimal Clusters (Elbow Method)\n",
    "# =====================================================================\n",
    "sse = []\n",
    "k_range = range(2, 11)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_processed)\n",
    "    sse.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(k_range, sse, marker='o')\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Sum of Squared Errors (SSE)\")\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.show()\n",
    "\n",
    "# =====================================================================\n",
    "# 4. Apply K-Means Clustering (Replace n_clusters with your choice)\n",
    "# =====================================================================\n",
    "n_clusters = 5  # Use the elbow/silhouette analysis to choose this\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(X_processed)\n",
    "\n",
    "# =====================================================================\n",
    "# 5. Evaluate Clusters\n",
    "# =====================================================================\n",
    "print(f\"Silhouette Score: {silhouette_score(X_processed, df['cluster']):.2f}\")\n",
    "\n",
    "# =====================================================================\n",
    "# 6. Visualize Clusters (2D PCA Projection)\n",
    "# =====================================================================\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_processed)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df['cluster'], cmap='viridis', alpha=0.6)\n",
    "plt.title(\"Behavioral Clusters (PCA Projection)\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# =====================================================================\n",
    "# 7. Interpret Clusters\n",
    "# =====================================================================\n",
    "# Get feature names after preprocessing\n",
    "numeric_names = preprocessor.named_transformers_['num'].named_steps['scaler'].get_feature_names_out(numerical_features)\n",
    "categorical_names = preprocessor.named_transformers_['cat'].named_steps['encoder'].get_feature_names_out(categorical_features)\n",
    "all_features = np.concatenate([numeric_names, categorical_names])\n",
    "\n",
    "# Create a DataFrame of processed features with cluster labels\n",
    "X_processed_df = pd.DataFrame(X_processed, columns=all_features)\n",
    "X_processed_df['cluster'] = df['cluster']\n",
    "\n",
    "# Analyze cluster characteristics\n",
    "cluster_summary = X_processed_df.groupby('cluster').mean().T\n",
    "print(cluster_summary)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
